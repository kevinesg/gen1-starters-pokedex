# Gen 1 Starters Pokedex
Object detection model for Gen 1 Pokemon starters. Final output is a Tensorflow Lite mobile app (Android only) but there's also a PC version.

Take note that I didn't include the raw images dataset, so you can't run `build_records.py` because the xml files won't be able to locate the images. But I included the resulting TFRecord files as if you were able to create it using `build_records.py`.

I will explain the step-by-step procedure I used from gathering images up to creating the TFLite mobile app. The steps are transferrable, if you want to apply it to your own dataset. Feel free to change directories of files but make sure to update the directories inside `config/C.py`.

## Contents
1. Generate dataset
2. Convert dataset into TFRecord files
3. Prepare files needed before model training
4. Start model training and evaluation
5. Monitor progress using Tensorboard
6. Convert model to TFLite format
7. Build a TFLite mobile app using our generated model

### 1. Generate dataset
#
The two main steps in generating your own dataset is (1) gathering images and (2) creating bounding boxes for each object within each image.

Obviously you can use your own method of gathering images from the web, but in this project I used [Image Dataset Tool](https://pypi.org/project/idt/).

Then I used [labelImg](https://pypi.org/project/labelImg/) to create the bounding boxes for each image. I attached an image below on how it looks like when using labelImg.

![image](https://user-images.githubusercontent.com/60960803/126108612-c44f2953-8960-4fba-893d-5c21d77397a4.png)

I chose the Pascal VOC format and the output is one xml file per image. After annotating all images, the next step is generating tfrecords files (train and test sets) using the images and xml files.

### 2. Convert dataset into TFRecord files
#
We need to create TFRecords files because the Tensorflow Object Detection library requires this format. It cannot read raw images and xml files.

To create TFRecord files containing our dataset, run `build_records.py`. Feel free to change directories as you wish. All directories are located at `config/C.py`.

This will generate train and test TFRecord files.

### 3. Prepare files needed before model training
#
At this point, you should choose a pre-trained model you will use for the transfer learning part. Check the available models at [TF2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). For this project, I tried two different models-- SSD MobileNet V2 FPNLite 640x640 and SSD MobileNet v2 FPNLite 320x320. I used SSD for faster training, and I chose the MobileNet variant for faster inference time (at the expense of slight loss in accuracy). My mobile phone is a 4-year-old model so I have to use a smaller and faster model.

After choosing a pre-trained model, download it and extract the files. Place it wherever you like (and don't forget to change the directory at `config/C.py`). For this project, I saved it at `model/pre-trained/`. Next, we need to configure `pipeline.config` to our own model and directories. I copied the `pipeline.config` file into `model/config/` and also renamed it to the pre-trained model name.

Open the config file and change `num_classes` to the number of classes (in my dataset, it is 4). Change `num_steps` into your desired number of steps. I used 100,000 steps but upon observation, around 50,000 steps would give the same results. Next, change `fine_tune_checkpoint_type` from "classification" to "detection", since we're doing object detection. `fine_tune_checkpoint` needs to point to the checkpoint folder of the pre-trained model. Then change `label_map_path` of the train and eval input to the directory of `classes.pbtxt` which was also generated by `build_records.py` earlier. Lastly, change the `input_path` of train and eval input to the directories of `training.record` and `testing.record`, respectively. Feel free to change any hyperparameter of the model, but for this project I didn't change anything else.

### 4. Start model training and evaluation
#
For this step, we will use Tensorflow 2 Object Detection library. Follow the installation instructions at the [official Tensorflow github](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md).

The first step in installing TF2OD is `git clone https://github.com/tensorflow/models.git`. Inside the downloaded folder, you can find a `research` folder. That's where we need to go, so open command prompt and change directory to the `research` folder, wherever it is on your system.

To train our model, we will use `object_detection`. In your opened command prompt, type

`python object_detection/model_main_tf2.py --pipeline_config_path={path to the config file that we edited in step 3} --model_dir={path to where you want to save the trained model files}  --alsologtostderr`

`model_main_tf2.py` is the code that we're running, `pipeline_config_path` points to the path of the config file we edited in step 3, `model_dir` points to the folder where you want to save the trained model files (in this project, I saved it inside `model/training/<name of model>`), and `alsologtostderr` is included so that we can track the progress using Tensorboard.

![image](https://user-images.githubusercontent.com/60960803/126121343-04b81697-dad8-41ed-a2c6-8a06594ad5e1.png)

After a while, you should see something similar to the photo above. It means training the model has started. Now we can start the evaluation. We don't need to wait the training process to finish before we can evaluate.

To evaluate our model, open up another command prompt (so we will have 2 command prompts running at the same time). Change the directory to the `research` folder again. We will use the exact script as before, we'll just add `--checkpoint_dir={path to the folder where you saved the trained model files}`. It's basically the same directory as `model_dir`. It will use the checkpoint files created by the training process.

![image](https://user-images.githubusercontent.com/60960803/126122241-a289ae21-e910-4f63-928c-9842a3989b48.png)

After a while, you should see something like this. It means the evaluation process is working as expected.

### 5. Monitor progress using Tensorboard
#
This next step is optional, but I recommend it. We will track the progress using Tensorboard. Open up a third command prompt. This time, we don't need to change our directory to the `research` folder. type `tensorboard --logdir={path to the folder where you saved the trained model files}`. It will give you a link (mine is http://localhost:6006/). Open it in your browser and you should see something like this:

![image](https://user-images.githubusercontent.com/60960803/126123570-e9cf6bac-42db-487c-b3d6-9afa8c4ec4d2.png)
![image](https://user-images.githubusercontent.com/60960803/126123712-31962417-1988-4fb2-a6e9-7826b151cb6b.png)

This is after 100,000 steps were completed, but you can monitor the current epoch of your live training process. In hindsight, only 50,000 to 60,000 steps were needed in this training process, as it gives the same accuracy and even lower losses.

Now that model training and evaluation is done, we can move on to the next step.

### 6. Convert model to TFLite format
#
The training process generated checkpoint files (saved to `--model_dir` directory). We will use it to create a TFLite-compatible model file. Like what we did earlier, open command prompt, change directory to the `research` folder, then type the script below. This time, we will use `export_tflite_graph_tf2.py`.

`python object_detection/export_tflite_graph_tf2.py --pipeline_config_path={path to the config file that we edited in step 3} --trained_checkpoint_dir={the same directory with model_dir earlier} --output_directory={path to where you want to save the tflite-compatible model}`

This will create a `saved_model` folder containing two folders (`assets` and `variables`) and the one we need, `saved_model.pb`.

Next, create a `labelmap.txt` wherever you like (and change its directory in `config/C.py`). This txt file should contain the classes in our model (take note of the arrangement). Mine is shown below.

![image](https://user-images.githubusercontent.com/60960803/126132854-b96001cf-5065-4d15-aa34-259ae9609112.png)

This `labelmap.txt` will be used in our next step.

To convert `saved_model.pb` into a tflite model, run `create_tflite.py` (don't forget to change `saved_model.pb` directory in `config/C.py`). It will generate two tflite files-- `model.tflite` and `detect.tflite`. What we want is the `detect.tflite`. It is basically the same as `model.tflite` but the metadata (from `labelmap.txt`) is already included. Basically, it means it's ready to be copied to build our own TFLite mobile app.

### 7. Build a TFLite mobile app using our generated model
#
First, we need to download and install [Android Studio](https://developer.android.com/studio). While installing Android Studio, download/git [this repo](https://github.com/tensorflow/examples.git) from Tensorflow. It contains good starting points not only for object detection but image classification, segmentation, etc. After installing Android Studio and downloading the repo, open Android Studio. Click File > Open then navigate `examples\lite\examples\object_detection\android\`. The `examples` folder is the folder from the Tensorflow repo we downloaded. Open `app/assets/` and delete the files inside. Copy the `detect.tflite` we created in step 6.

![image](https://user-images.githubusercontent.com/60960803/126129066-13b8083a-3c55-4509-a6a0-8e651233302b.png)

Lastly, open `build.gradle` and comment out this line

![image](https://user-images.githubusercontent.com/60960803/126129208-73e70485-3d6f-4451-934e-4444101abf76.png)

We do this when we use our own custom `detect.tflite` models.

Now we're ready! Click Build > Build Bundle(s) and APK(s) > Build APK(s). If it is grayed out, you might need to click Build > Rebuild Project first.

![image](https://user-images.githubusercontent.com/60960803/126129597-1742a215-f7cc-457b-b4fd-b95e84c72cb9.png)

After a few seconds, you will get a notification. You can locate the .apk file which you can copy to your Android phone.

![image](https://user-images.githubusercontent.com/60960803/126129858-684b56ea-ab17-4a7d-9218-6fab877da89b.png)

Another option is connecting your phone to your PC and clicking the Run button so it will directly install the .apk file to your phone. But this step requires additional steps like enabling Developer Options, etc. For more info, feel free to check https://developer.android.com/studio/run/device.

Here are some screenshots using the TFLite app I compiled:

![image](https://user-images.githubusercontent.com/60960803/126131462-eb7c0353-3961-4703-a54c-2bd26c7366c5.png)

#

![image](https://user-images.githubusercontent.com/60960803/126131538-558165e5-60c0-4251-a515-57169da1465d.png)

As we can see, it takes around half a second to identify and locate the Pokemon. But this is partly due to my 4-year-old phone. If you use a faster phone, it can take as fast as 0.1 sec.

#

We can also use a PC version which can either read a video file or use your web cam.

To do this, we will "go back" to step 6 but instead of creating a TFLite-compatible model, we will create a model that we can open using our PC. As usual, open command prompt and change directory to the `research` folder. We will use `exporter_main_v2.py`. Type the code below.

`python object_detection/exporter_main_v2.py --input_type=image_tensor --pipeline_config_path={path to the config file that we edited in step 3} --trained_checkpoint_dir={the same directory with model_dir earlier} --output_directory={path to where you want to save the PC-compatible model}`

This will create a `saved_model` folder.

Now you can run `predict_vid.py`. Note that this requires the correct path of the `saved_model` folder (change it inside `config/C.py`). It takes ~30 seconds on my machine to load the saved model (GPU: RTX 2060) so as long as you're not encountering error messages, please wait a bit. You can set the `USE_CAM` flag inside `predict_vid.py` to `True` if you want it to use your web cam. Otherwise, set it to `False` and provide the correct `VIDEO_FILE` path that you want to open.

https://user-images.githubusercontent.com/60960803/126135978-024f2478-f335-4a9b-81d0-45923a23806a.mp4

Here is a short clip showing how it performs on my PC. (Press Q to close the video and stop `predict_vid.py`)

#

If you have any questions or suggestions, feel free to contact me here. Thank you for reading!
